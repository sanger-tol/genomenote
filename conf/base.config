/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    sanger-tol/genomenote Nextflow base config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
*/

process {

    errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
    maxRetries    = 5
    maxErrors     = '-1'

    // In this configuration file, we give little resources by default and
    // explicitly bump them up for some processes.
    // All rules should still increase resources every attempt to allow the
    // pipeline to self-heal from MEMLIMIT/RUNLIMIT.

    // Default
    cpus   = 1
    memory = { check_max( 50.MB * task.attempt, 'memory' ) }
    time   = { check_max( 30.min * task.attempt, 'time' ) }

    // These processes typically complete within 30 min to 1.5 hours.
    withName: 'BED_SORT|BEDTOOLS_BAMTOBED|COOLER_CLOAD|COOLER_ZOOMIFY|FILTER_BED' {
        time   = { check_max( 4.hour * task.attempt, 'time' ) }
    }

    // These processes may take a few hours.
    withName: 'FILTER_SORT|SAMTOOLS_VIEW' {
        time   = { check_max( 8.hour * task.attempt, 'time' ) }
    }

    withName: SAMTOOLS_VIEW {
        memory = { check_max( 1.GB  * task.attempt, 'memory'  ) }
    }

    withName: FASTK_FASTK {
        memory = { check_max( 12.GB * task.attempt, 'memory'  ) }
        cpus   = { log_increase_cpus(4, 2*task.attempt, 1, 2) }
    }

    withName: MERQURYFK_MERQURYFK {
        // Memory usage seems to be following two different linear rules:
        //  - 1 GB for every 60 Mbp for genomes < 840 Mbp
        //  - 2 GB for every 1 Gbp for genomes > 840 Mbp, with a 12 GB offset to match the previous rule
        memory = { check_max( 1.GB + ((meta.genome_size < 840000000) ? (Math.ceil(meta.genome_size/60000000) * 1.GB * task.attempt) : (Math.ceil(meta.genome_size/1000000000) * 2.GB * task.attempt + 12.GB)), 'memory' ) }
        cpus   = { log_increase_cpus(4, 2*task.attempt, 1, 2) }
    }

    withName: BUSCO {
        // No straightforward formula, so using ranges instead.
        // The memory is increased by half of the base value at every attempt.
        memory = { check_max( (
                        meta.genome_size <  100000000 ?  4.GB :
                        meta.genome_size <  500000000 ?  8.GB :
                        meta.genome_size < 1000000000 ? 16.GB :
                        meta.genome_size < 2000000000 ? 32.GB :
                        meta.genome_size < 5000000000 ? 64.GB : 192.GB
                    ) * ((task.attempt+1)/2) , 'memory' ) }
        cpus   = { log_increase_cpus(4, 2*task.attempt, Math.ceil(meta.genome_size/1000000000), 2) }
        time   = { check_max( 2.h * Math.ceil(meta.genome_size/1000000000) * task.attempt, 'time') }
    }

    withName: 'BED_SORT|FILTER_SORT' {
        cpus   = { log_increase_cpus(2, 2*task.attempt, 1, 2) }
        memory = { check_max( 16.GB * task.attempt, 'memory'  ) }
    }

    withName: COOLER_CLOAD {
        memory = { check_max( 6.GB  * task.attempt, 'memory'  ) }
    }

    withName: COOLER_DUMP {
        memory = { check_max( 100.MB * task.attempt, 'memory'  ) }
    }

    withName: COOLER_ZOOMIFY {
        cpus   = { log_increase_cpus(2, 2*task.attempt, 1, 2) }
        memory = { check_max( (meta.genome_size < 1000000000 ? 16.GB : 24.GB) * task.attempt, 'memory' ) }
    }

    withName: MULTIQC {
        memory = { check_max( 150.MB  * task.attempt, 'memory'  ) }
    }

    withName:CUSTOM_DUMPSOFTWAREVERSIONS {
        cache = false
    }
    withName: AGAT_SQSTATBASIC {
        memory = { check_max( 300.MB  * task.attempt, 'memory'  ) }
    }

    withName: AGAT_SPSTATISTICS {
        memory = { check_max( 300.MB  * task.attempt, 'memory'  ) }
    }
}
